---以下はChatGPTによる提案---
1) 問題定義と採点仕様（プロダクト要件）

1-1. 9軸100点ルーブリックの最終版

軸・重み：明確性10 / 証拠20 / 合意15 / 機序10 / 透明性10 / 文脈歪曲10 / 害15 / 拡散5 / 訂正5

ラベル閾値：90–100 正確 / 75–89 ほぼ正確 / 55–74 根拠薄い/ミスリード / 35–54 不正確 / 0–34 事実無根

それぞれの採点基準の判定例（OK/NGの境界サンプル文）を各軸2〜3本ずつ用意

1-2. スコア出力の契約（API/CLI仕様）

入力：claim_text, source_url(or text_id), topic, lang

出力：total_score, axis_scores{}, label, rationales[], evidence_top3[](pmid/url/stance)

SLA：1件あたり応答 ≤ 3s（キャッシュ有）、タイムアウト時は安全側の“根拠不足”で返す

1-3. 透明性仕様

各判定に根拠URL最低1本（PubMed/学会/公的機関）

**ClaimReview（JSON-LD）**のフィールドセットを固定（claimReviewed, reviewRating, itemReviewed, url, datePublished など）

2) データと評価（アノテ基盤）

2-1. シード・データ（最小でOK）

日本語の健康関連“主張” 50–100件（反ワク、トンデモ医療、反農薬/オーガニック、対照群）

各主張に人手ラベル（9軸素点＋最終ラベル）と根拠リンクを1–3本

フォーマット：JSONL（1行=1主張）。{claim_text, topic, source_url, gold:{axis_scores, label, evidence[]}}

2-2. 評価指標の合意

分類：False/Misleading 検出F1（目標≥0.75）

校正：Brier Score（≤0.18）

再現性：評定者間ICC（≥0.75）

失敗時のエラー種別タグ（例：相関→因果誤認、文脈逸脱、古い研究の過剰一般化）

3) アーキテクチャと環境（実装の土台）

3-1. 技術スタック（MVP想定）

言語：Python 3.11 以上

API：FastAPI

形態素/日本語NLP：GiNZA or SudachiPy（どちらかに統一）

文章ベクトル：sentence-transformers（多言語）

NLI（自然言語推論）：多言語NLIの事前学習モデル（ローカル or API切替可能）

DB：PostgreSQL（本番）/ SQLite（開発）。類似検索はpgvector or FAISS

キャッシュ：Redis（任意）

3-2. サービス構成

ingest（収集） → extract（主張抽出） → evidence（検索・要約） → nli（支持/反証判定） → score（9軸集計） → publish（ClaimReview生成）

3-3. 設定と鍵

.env：NCBI_EMAIL, NCBI_API_KEY(任意), OPENAI_OR_HF_KEY(任意), DB_URL

レート制御：外部APIはキュー/バックオフ内蔵（失敗時のフォールバック“根拠不足”）

3-4. 依存関係管理と再現性

Poetry or uv（推奨）でlock

Dockerfile（CPU前提）＋docker-compose（DB/Redis込み）

seed データ投入用 Make タスク

4) モジュールごとの「完成の定義（DoD）」

4-1. 抽出（extract）

目的：テキストから“因果・効能・安全性”の主張文を抜き出す

DoD：開発用ゴールドで再現率≥0.85 / 過検出（ノイズ）≤15%

仕様：ルール（正規表現・依存構造パターン）＋軽量分類器のハイブリッド

4-2. エビデンス（evidence）

目的：ガイドライン/系統的レビュー/大規模研究を上位にTop-N抽出し要約

DoD：Top-3のうち最低1本が人手根拠と一致（Hit@3 ≥ 0.8）

仕様：PICO風クエリ生成、PubMed/E-utilities、学会/省庁サイトの優先ドメイン

4-3. NLI

目的：主張 vs. エビデンス要旨の支持/反証/中立判定

DoD：開発セットで Macro-F1 ≥ 0.75、推論≤300ms/ペア（GPUなし）

仕様：多言語NLIモデル＋日本語要旨（抽出or要約）

4-4. スコアリング（score）

目的：9軸素点→合計→ラベル

DoD：ゴールドとのMAE ≤ 7点、最終ラベル一致率 ≥ 0.8

仕様：ルール（危険フレーズ辞書）＋NLI結果＋メタ情報（研究デザイン/年）を重み付け

4-5. 公開（publish）

目的：人が読める結論・根拠・代替説明、ClaimReview JSON-LDを自動生成

DoD：構造化データ検証OK、リンク生存率100%、更新履歴表示

5) 法務・倫理・運用（安全装置）

対象は“主張”であって人物ではない（文面テンプレを事前決定）

不確実時は**“Unsupported/根拠不足”**へ倒す

引用は必要最小限（抜粋・要約中心）、著作権とTOSを順守

異議申し立てフォーム（簡易でOK）と更新記録を保持

6) バイブコーディングの進め方（セッション設計）

セッション0（30–45分）

ルーブリック最終確認、API入出力の固定、seed JSONLの受け渡し

セッション1（60–90分）

プロジェクト雛形作成（FastAPI, Poetry, Docker, DBスキーマ）

/score のスタブ実装（ダミー得点＋ダミーevidence返却まで）

セッション2（60–90分）

extract 実装（日本語パターン＋簡易分類器）、単体テスト

evidence のPubMed検索→上位要旨の取得まで

セッション3（60–90分）

nli（事前学習モデルの推論パイプ）＋スコア統合ロジック

人手ゴールドで初回評価→重み・閾値の微調整

セッション4（60–90分）

publish（ClaimReview生成・ミニUI）

回帰テスト、最初の10件を公開可能レベルに

以降は、誤差分析→辞書/重み更新→学会・公的機関ドメインの追加最適化、の反復。

7) 事前に用意しておいてほしいもの（チェックリスト）

 シード主張 50–100件（JSONL）と、あなたの金標スコア（ざっくりでOK）

 根拠URL（PubMed/学会）を各主張1–3本

 9軸ルーブリックPDF（最終版）

 .env に入れる鍵/設定（ダミー可）

 Dockerが動くマシン（CPUでOK、メモリ16GB目安）

 “公開時の文言テンプレ”ドラフト（「根拠不十分」「文脈により誤解」など）

8) 初回セッションの成果物イメージ（合意しておくと早い）

FastAPI プロジェクト雛形＋/score スタブ

claims/evidence/scores のDBテーブル定義

抽出・検索・NLIのモジュール境界（関数シグネチャ）

単体テストの最初の5ケース（失敗時ふるまいを含む）
---